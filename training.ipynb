{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– Artificial Intelligence, Machine Learning and Python ðŸ¤–"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does it all matter? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Intelligence is becoming more and more prevalent in healthcare. Look at these recent headlines! \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](.//headline_1.png)\n",
    "\n",
    "![image](.//headline_2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Intelligence is going to become more and more prominent within the NHS."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](.//headline_3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, Artificial Intelligence is just really interesting! (And sometimes a bit scary!) It's here to stay, so understanding it is going to be important for those doing testing and assurance in the NHS. \n",
    "Here are a final selection of headlines to show just how **versatile**, **interesting**, and **important** AI is becoming."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](.//headline_4.png)\n",
    "\n",
    "![image](.//headline_5.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is todays plan? \n",
    "\n",
    "##### **We will start by creating an Artificial Intelligence that can predict if an individual has breast cancer?**\n",
    "##### **We will then learn a few ways of testing and assuring Artificial intellegence models** \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we get into coding - let's go over some definitions.\n",
    "\n",
    "Firstly - what actually is Artificial Intelligence? \n",
    "\n",
    "Online Chatbots, Amazon Alexa, a self driving Tesla, these are all examples of **machine learning** and **artificial intelligence**. \n",
    "\n",
    "- Artificial Intelligence (often referred to as AI) is a general term used to describe computers completing task that we would consider clever or intelligent. \n",
    "\n",
    "- Machine Learning is a particular application of AI, it is where computers use data to learn patterns and make predictions without explicit instructions from developers. \n",
    "\n",
    "Today, our focus is going to be more closely aligned to machine learning. We are going to use a dataset to **train** a **machine learning model** (often just referred to as a model), so that it can make **predictions** that we can **test** and **assure**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Recap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's give a quick recap of python and of functions. This will help us a lot later! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n"
     ]
    }
   ],
   "source": [
    "# This is a block of code written in python. It is written inside a box, known as a cell.\n",
    "# Every time you get to a piece of code in this notebook, you will need to run it.\n",
    "# There are two possible ways to do this.\n",
    "\n",
    "# 1 - Click the triangular play button in the top left corner of the cell. \n",
    "# 2 - If you are clicked inside the cell, you can hold shift and press enter.\n",
    "\n",
    "# Once the code has successfully ran, you will see a little blue tick.\n",
    "\n",
    "# Sometimes, the code will also give an output, which will also appear when you have ran the code. \n",
    "# Like this!\n",
    "print(\"Hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a very simple function that works out percentages.\n",
    "# You pass it in two numbers, called parameters.\n",
    "# These represent the numerator and denominator of a fraction.\n",
    "# Some simple maths is done, and the fraction is returned.  \n",
    "\n",
    "def calculate_percentage(numerator, denominator):\n",
    "    fraction = numerator / denominator\n",
    "    return fraction * 100\n",
    "\n",
    "# When you run this code snippet nothing will happen, you need to call the function for it to return you something.\n",
    "# You should still see a little green tick!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Call the function here:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# You should see a output when you run this piece of code.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# If you get an error, make sure you have entered two numbers without speech marks between the brackets separated by a comma. \u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m calculate_percentage(\u001b[39m\"\"\"\u001b[39;49m\u001b[39myour value 1\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m, \u001b[39m\"\"\"\u001b[39;49m\u001b[39myour value 2\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m, in \u001b[0;36mcalculate_percentage\u001b[0;34m(numerator, denominator)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_percentage\u001b[39m(numerator, denominator):\n\u001b[0;32m----> 7\u001b[0m     fraction \u001b[39m=\u001b[39m numerator \u001b[39m/\u001b[39;49m denominator\n\u001b[1;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m fraction \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "# Call the function here:\n",
    "# You should see a output when you run this piece of code.\n",
    "# If you get an error, make sure you have entered two numbers without speech marks between the brackets separated by a comma. \n",
    "\n",
    "calculate_percentage(\"\"\"your value 1\"\"\", \"\"\"your value 2\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets look at some data! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the next few steps, we are going to follow the path of a data scientist creating a machine learning model.\n",
    "\n",
    "**Don't worry!** This is not something you will be expected to do after this course. It is just an opportunity to learn how a machine learning model is made, and this will help you understand why we test it in certain ways later in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pandas in /usr/local/lib/python3.8/site-packages (1.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/Ben/Library/Python/3.8/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.8/site-packages (from pandas) (1.21.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Ben/Library/Python/3.8/lib/python/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: plotly in /usr/local/lib/python3.8/site-packages (5.15.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.8/site-packages (from plotly) (8.2.2)\n",
      "Requirement already satisfied: packaging in /Users/Ben/Library/Python/3.8/lib/python/site-packages (from plotly) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/Ben/Library/Python/3.8/lib/python/site-packages (from packaging->plotly) (3.0.9)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: shap in /usr/local/lib/python3.8/site-packages (0.42.1)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.8/site-packages (from shap) (0.55.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/site-packages (from shap) (1.8.0)\n",
      "Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.8/site-packages (from shap) (0.0.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/site-packages (from shap) (1.1.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/site-packages (from shap) (1.21.2)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/site-packages (from shap) (2.2.1)\n",
      "Requirement already satisfied: packaging>20.9 in /Users/Ben/Library/Python/3.8/lib/python/site-packages (from shap) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.8/site-packages (from shap) (4.64.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/site-packages (from shap) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/Ben/Library/Python/3.8/lib/python/site-packages (from packaging>20.9->shap) (3.0.9)\n",
      "Requirement already satisfied: setuptools in /Users/Ben/Library/Python/3.8/lib/python/site-packages (from numba->shap) (63.1.0)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /usr/local/lib/python3.8/site-packages (from numba->shap) (0.38.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.8/site-packages (from pandas->shap) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/Ben/Library/Python/3.8/lib/python/site-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/site-packages (from scikit-learn->shap) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/site-packages (from scikit-learn->shap) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Ben/Library/Python/3.8/lib/python/site-packages (from python-dateutil>=2.7.3->pandas->shap) (1.16.0)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /usr/local/lib/python3.8/site-packages (1.3.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.8/site-packages (from scikit-learn) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/site-packages (from scikit-learn) (1.21.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/site-packages (from scikit-learn) (1.3.1)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# # These next few lines of code install certain packages. These contain lots and lots of functions which we can use in our code later.\n",
    "\n",
    "!pip install pandas\n",
    "!pip install plotly\n",
    "!pip install -U shap\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post7.tar.gz (3.6 kB)\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /Users/Ben/INSTANT/Instant_Training/.venv/bin/python3 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/p4/d8v6xgfn5279fl17t5s538v80000gp/T/pip-install-5mbpp45t/sklearn_5bfbd9ebfa814042ba582ef7c414aa3e/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/p4/d8v6xgfn5279fl17t5s538v80000gp/T/pip-install-5mbpp45t/sklearn_5bfbd9ebfa814042ba582ef7c414aa3e/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /private/var/folders/p4/d8v6xgfn5279fl17t5s538v80000gp/T/pip-pip-egg-info-7znr7m5h\n",
      "         cwd: /private/var/folders/p4/d8v6xgfn5279fl17t5s538v80000gp/T/pip-install-5mbpp45t/sklearn_5bfbd9ebfa814042ba582ef7c414aa3e/\n",
      "    Complete output (18 lines):\n",
      "    The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "    rather than 'sklearn' for pip commands.\n",
      "    \n",
      "    Here is how to fix this error in the main use cases:\n",
      "    - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "    - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "      (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "    - if the 'sklearn' package is used by one of your dependencies,\n",
      "      it would be great if you take some time to track which package uses\n",
      "      'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "    - as a last resort, set the environment variable\n",
      "      SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "    \n",
      "    More information is available at\n",
      "    https://github.com/scikit-learn/sklearn-pypi-package\n",
      "    \n",
      "    If the previous advice does not cover your use case, feel free to report it at\n",
      "    https://github.com/scikit-learn/sklearn-pypi-package/issues/new\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/70/ce/81aa643f3c43488c4a1e417e45f696a61e7ac82b57190fad3c310df2c07b/sklearn-0.0.post7.tar.gz#sha256=1c89020b364fdc3aa2839e0ae34e8f0b406669e4b5c2359dda3ac398f9c76874 (from https://pypi.org/simple/sklearn/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "  Using cached sklearn-0.0.post5.tar.gz (3.7 kB)\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /Users/Ben/INSTANT/Instant_Training/.venv/bin/python3 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/p4/d8v6xgfn5279fl17t5s538v80000gp/T/pip-install-5mbpp45t/sklearn_16582dc11d2b4016966682835b0fe08d/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/p4/d8v6xgfn5279fl17t5s538v80000gp/T/pip-install-5mbpp45t/sklearn_16582dc11d2b4016966682835b0fe08d/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /private/var/folders/p4/d8v6xgfn5279fl17t5s538v80000gp/T/pip-pip-egg-info-tlgd9cny\n",
      "         cwd: /private/var/folders/p4/d8v6xgfn5279fl17t5s538v80000gp/T/pip-install-5mbpp45t/sklearn_16582dc11d2b4016966682835b0fe08d/\n",
      "    Complete output (18 lines):\n",
      "    The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "    rather than 'sklearn' for pip commands.\n",
      "    \n",
      "    Here is how to fix this error in the main use cases:\n",
      "    - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "    - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "      (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "    - if the 'sklearn' package is used by one of your dependencies,\n",
      "      it would be great if you take some time to track which package uses\n",
      "      'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "    - as a last resort, set the environment variable\n",
      "      SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "    \n",
      "    More information is available at\n",
      "    https://github.com/scikit-learn/sklearn-pypi-package\n",
      "    \n",
      "    If the previous advice does not cover your use case, feel free to report it at\n",
      "    https://github.com/scikit-learn/sklearn-pypi-package/issues/new\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/7a/93/e0e1b1e98f39dfca7ec9795cb46f6e09e88a2fd5d4a28e4b3d1f618a2aec/sklearn-0.0.post5.tar.gz#sha256=7377c714a03a79bbe9196f435db931fd2a6fa8c68514da7ed3a251fd08c52e2c (from https://pypi.org/simple/sklearn/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "  Downloading sklearn-0.0.post4.tar.gz (3.6 kB)\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /Users/Ben/INSTANT/Instant_Training/.venv/bin/python3 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/p4/d8v6xgfn5279fl17t5s538v80000gp/T/pip-install-5mbpp45t/sklearn_5dc687ac8387428ca3f3bde3f7f71109/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/p4/d8v6xgfn5279fl17t5s538v80000gp/T/pip-install-5mbpp45t/sklearn_5dc687ac8387428ca3f3bde3f7f71109/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /private/var/folders/p4/d8v6xgfn5279fl17t5s538v80000gp/T/pip-pip-egg-info-uykyao4h\n",
      "         cwd: /private/var/folders/p4/d8v6xgfn5279fl17t5s538v80000gp/T/pip-install-5mbpp45t/sklearn_5dc687ac8387428ca3f3bde3f7f71109/\n",
      "    Complete output (18 lines):\n",
      "    The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "    rather than 'sklearn' for pip commands.\n",
      "    \n",
      "    Here is how to fix this error in the main use cases:\n",
      "    - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "    - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "      (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "    - if the 'sklearn' package is used by one of your dependencies,\n",
      "      it would be great if you take some time to track which package uses\n",
      "      'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "    - as a last resort, set the environment variable\n",
      "      SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "    \n",
      "    More information is available at\n",
      "    https://github.com/scikit-learn/sklearn-pypi-package\n",
      "    \n",
      "    If the previous advice does not cover your use case, feel free to report it at\n",
      "    https://github.com/scikit-learn/sklearn-pypi-package/issues/new\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/99/b2/165110013aa66fae6fc13918ad0e9de4801e5f1691d371bf8b63328037e6/sklearn-0.0.post4.tar.gz#sha256=0e81ec9c32d4bb418e7be8f1ec1027d174975502dc84cbc4f4564b4cba31e674 (from https://pypi.org/simple/sklearn/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /Users/Ben/INSTANT/Instant_Training/.venv/bin/python3 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/p4/d8v6xgfn5279fl17t5s538v80000gp/T/pip-install-5mbpp45t/sklearn_663447b2a7b74d5facd0fcec1bd66e4e/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/p4/d8v6xgfn5279fl17t5s538v80000gp/T/pip-install-5mbpp45t/sklearn_663447b2a7b74d5facd0fcec1bd66e4e/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /private/var/folders/p4/d8v6xgfn5279fl17t5s538v80000gp/T/pip-pip-egg-info-v7n2iv0e\n",
      "         cwd: /private/var/folders/p4/d8v6xgfn5279fl17t5s538v80000gp/T/pip-install-5mbpp45t/sklearn_663447b2a7b74d5facd0fcec1bd66e4e/\n",
      "    Complete output (18 lines):\n",
      "    The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "    rather than 'sklearn' for pip commands.\n",
      "    \n",
      "    Here is how to fix this error in the main use cases:\n",
      "    - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "    - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "      (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "    - if the 'sklearn' package is used by one of your dependencies,\n",
      "      it would be great if you take some time to track which package uses\n",
      "      'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "    - as a last resort, set the environment variable\n",
      "      SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "    \n",
      "    More information is available at\n",
      "    https://github.com/scikit-learn/sklearn-pypi-package\n",
      "    \n",
      "    If the previous advice does not cover your use case, feel free to report it at\n",
      "    https://github.com/scikit-learn/sklearn-pypi-package/issues/new\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/db/1e/af4e9cded5093a92e60d4ae7149a02c7427661b2db66c8ea4d34b17864a2/sklearn-0.0.post1.tar.gz#sha256=76b9ed1623775168657b86b5fe966d45752e5c87f528de6240c38923b94147c5 (from https://pypi.org/simple/sklearn/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "  Using cached sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.0.3-cp310-cp310-macosx_10_9_x86_64.whl (11.8 MB)\n",
      "Collecting plotly\n",
      "  Using cached plotly-5.15.0-py2.py3-none-any.whl (15.5 MB)\n",
      "Collecting shap\n",
      "  Downloading shap-0.42.1-cp310-cp310-macosx_10_9_x86_64.whl (465 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 465 kB 2.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-1.25.2-cp310-cp310-macosx_10_9_x86_64.whl (20.8 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.8 MB 12.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.0-cp310-cp310-macosx_10_9_x86_64.whl (10.2 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.2 MB 22.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Using cached tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from plotly->-r requirements.txt (line 3)) (23.1)\n",
      "Collecting slicer==0.0.7\n",
      "  Using cached slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Collecting numba\n",
      "  Using cached numba-0.57.1-cp310-cp310-macosx_10_9_x86_64.whl (2.5 MB)\n",
      "Collecting cloudpickle\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting tqdm>=4.27.0\n",
      "  Downloading tqdm-4.66.0-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78 kB 9.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting scipy\n",
      "  Using cached scipy-1.11.1-cp310-cp310-macosx_10_9_x86_64.whl (37.2 MB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 2)) (1.16.0)\n",
      "Collecting llvmlite<0.41,>=0.40.0dev0\n",
      "  Using cached llvmlite-0.40.1-cp310-cp310-macosx_10_9_x86_64.whl (30.4 MB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.24.4-cp310-cp310-macosx_10_9_x86_64.whl (19.8 MB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 302 kB 15.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Using legacy 'setup.py install' for sklearn, since package 'wheel' is not installed.\n",
      "Installing collected packages: numpy, tzdata, threadpoolctl, scipy, pytz, llvmlite, joblib, tqdm, tenacity, slicer, scikit-learn, pandas, numba, cloudpickle, sklearn, shap, plotly\n",
      "    Running setup.py install for sklearn ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed cloudpickle-2.2.1 joblib-1.3.2 llvmlite-0.40.1 numba-0.57.1 numpy-1.24.4 pandas-2.0.3 plotly-5.15.0 pytz-2023.3 scikit-learn-1.3.0 scipy-1.11.1 shap-0.42.1 sklearn-0.0 slicer-0.0.7 tenacity-8.2.2 threadpoolctl-3.2.0 tqdm-4.66.0 tzdata-2023.3\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/Users/Ben/INSTANT/Instant_Training/.venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have installed them, we can import certain libraries and functions. \n",
    "\n",
    "# This first function is used to load a pre-existing breast cancer dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Pandas is a library containing lots of functions used to modify and navigate dataframes.\n",
    "import pandas as pd\n",
    "\n",
    "# Plotly is a library which helps us generate plots.\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are about to use the load_breast_cancer function from sklearn \n",
    "# This calls the function to return:\n",
    "\n",
    "#breast_cancer_inputs  \n",
    "# - This is the raw data containing information about different breast cancer screenings\n",
    "\n",
    "#breast_cancer_outputs \n",
    "# - This contains the classification of whether each screening is identified as being breast cancer\n",
    "\n",
    "breast_cancer_inputs, breast_cancer_outputs = load_breast_cancer(return_X_y=True, as_frame=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "0                   0.07871  ...        25.380          17.33   \n",
       "1                   0.05667  ...        24.990          23.41   \n",
       "2                   0.05999  ...        23.570          25.53   \n",
       "3                   0.09744  ...        14.910          26.50   \n",
       "4                   0.05883  ...        22.540          16.67   \n",
       "..                      ...  ...           ...            ...   \n",
       "564                 0.05623  ...        25.450          26.40   \n",
       "565                 0.05533  ...        23.690          38.25   \n",
       "566                 0.05648  ...        18.980          34.12   \n",
       "567                 0.07016  ...        25.740          39.42   \n",
       "568                 0.05884  ...         9.456          30.37   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "0             184.60      2019.0           0.16220            0.66560   \n",
       "1             158.80      1956.0           0.12380            0.18660   \n",
       "2             152.50      1709.0           0.14440            0.42450   \n",
       "3              98.87       567.7           0.20980            0.86630   \n",
       "4             152.20      1575.0           0.13740            0.20500   \n",
       "..               ...         ...               ...                ...   \n",
       "564           166.10      2027.0           0.14100            0.21130   \n",
       "565           155.00      1731.0           0.11660            0.19220   \n",
       "566           126.70      1124.0           0.11390            0.30940   \n",
       "567           184.60      1821.0           0.16500            0.86810   \n",
       "568            59.16       268.6           0.08996            0.06444   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "0             0.7119                0.2654          0.4601   \n",
       "1             0.2416                0.1860          0.2750   \n",
       "2             0.4504                0.2430          0.3613   \n",
       "3             0.6869                0.2575          0.6638   \n",
       "4             0.4000                0.1625          0.2364   \n",
       "..               ...                   ...             ...   \n",
       "564           0.4107                0.2216          0.2060   \n",
       "565           0.3215                0.1628          0.2572   \n",
       "566           0.3403                0.1418          0.2218   \n",
       "567           0.9387                0.2650          0.4087   \n",
       "568           0.0000                0.0000          0.2871   \n",
       "\n",
       "     worst fractal dimension  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "..                       ...  \n",
       "564                  0.07115  \n",
       "565                  0.06637  \n",
       "566                  0.07820  \n",
       "567                  0.12400  \n",
       "568                  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What does the data look like?\n",
    "display(breast_cancer_inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's be honest, this data is really confusing and we cannot really tell anything from it!\n",
    "\n",
    "We can create a plotting function to help us spot some trends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_scatter_plot(input_dataframe: pd.DataFrame,\n",
    "                         output_series: pd.Series,\n",
    "                         columns:list):\n",
    "    \"\"\"Produces a 2D or 3D scatter plot based on columns from dataframe.\n",
    "\n",
    "    Args:\n",
    "        input_dataframe (pd.DataFrame): The dataframe containing breast cancer information\n",
    "        output_series (pd.Series): The dataframe containing diagnosis information\n",
    "        columns (list): The columns to plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate a series of strings to use as a colour key\n",
    "    colours = pd.Series(str(value) for value in output_series)\n",
    "    \n",
    "    #Check the length of the columns.\n",
    "    # If length is 3, make a 3D plot.\n",
    "    # If length is 2, make a 2D plot.\n",
    "    if len(columns) == 3:\n",
    "        fig = px.scatter_3d(input_dataframe, x=columns[0], y=columns[1], z=columns[2], color=colours, labels={\"color\": \"Diagnosis\", \"Symbol\": \"Diagnosis\"})\n",
    "    elif len(columns) == 2:\n",
    "        fig = px.scatter(input_dataframe, x=columns[0], y=columns[1], color =colours, labels={\"color\": \"Diagnosis\", \"Symbol\": \"Diagnosis\"})\n",
    "    elif len(columns) != (2 or 3):\n",
    "        print(\"Please only use 2 or 3 columns\")\n",
    "        return None\n",
    "\n",
    "    # Update the graphics\n",
    "    fig.update_traces(marker=dict(size = 4 if len(columns) == 3 else 10,\n",
    "                              line=dict(width=2, color='DarkSlateGrey')),\n",
    "                              selector=dict(mode='markers'))\n",
    "    \n",
    "    fig.show()\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](.//task.png)\n",
    "\n",
    "This plotting function can be called by passing into it breast_cancer_inputs, breast_cancer_outputs, and an array of column names.\n",
    "\n",
    "Try using the column names to generate some 2D and 3D plots, see if you see any trends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remind ourselves of the column names\n",
    "print(breast_cancer_inputs.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_scatter_plot(breast_cancer_inputs,\n",
    "                     breast_cancer_outputs,\n",
    "                     ['mean radius','mean texture']) # <== Change these two column names to any two from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'produce_scatter_plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m produce_scatter_plot(breast_cancer_inputs,\n\u001b[1;32m      2\u001b[0m                      breast_cancer_outputs,\n\u001b[1;32m      3\u001b[0m                      []) \u001b[39m# Try putting in three column names here\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'produce_scatter_plot' is not defined"
     ]
    }
   ],
   "source": [
    "produce_scatter_plot(breast_cancer_inputs,\n",
    "                     breast_cancer_outputs,\n",
    "                     []) # Try putting in three column names here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 30 different columns within this dataset, meaning there are 439 different pairs you could plot in a 2D graph. Spotting patterns, or more importantly the most important patterns, is an incredibly complex task. \n",
    "\n",
    "Further to this, can you imagine having a 30 dimensional scatter plot to try and spot patterns. For us, it is completely impossible! But for a machine, this is something it can do easily. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can we use this data in a machine learning model? Yes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# When creating a machine learning model, it is important to have some training data and some testing data.\n",
    "\n",
    "# Training data - this is data that is used to train the model.\n",
    "# Testing data - this is data used to test the model, we will use this a lot later!\n",
    "\n",
    "# The following function splits our data randomly to test and training data.\n",
    "breast_cancer_inputs_training, breast_cancer_inputs_testing, breast_cancer_outputs_training, breast_cancer_outputs_testing = train_test_split(breast_cancer_inputs, breast_cancer_outputs, train_size=0.8)\n",
    "\n",
    "# train_size represents the proportion of data that should be used for training\n",
    "# we have gone for 80% by setting train_size to 0.8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are multiple types of machine learning algorithms: \n",
    "\n",
    "**Classification algorithms** are machine learning techniques used to predict categorical labels or classes based on input data.\n",
    "\n",
    "**Regression algorithms** are machine learning techniques used to predict continuous numerical values based on input data.\n",
    "\n",
    "**Clustering algorithms** are machine learning techniques used to group similar data points together based on their inherent similarities, without any predefined labels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](.//task.png)\n",
    "\n",
    "#### Can you tell which type of algorithm we want to use?\n",
    "\n",
    "Uncomment (remove the #) the correct type of model we want to use for our training in the code below.\n",
    "Don't scroll down too far or you will see the answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant models\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "classification_model = RandomForestClassifier()\n",
    "# regression_model = RandomForestRegressor()\n",
    "# cluster_model = KMeans()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a model, our next step is to train it.\n",
    "\n",
    "sklearn has an incredibly useful function - `fit` \n",
    "\n",
    "The fit method trains the algorithm on the data.\n",
    "\n",
    "How it does this depends on the model. If you would like to know more about the random forest algorithm you can read about it here: https://www.turing.com/kb/random-forest-algorithm. But, a lot of this specific knowledge you wont need to know today.\n",
    "\n",
    "**Importantly**: All algorithms with labelled training data use a loss function to train a predictive function. They are all dependent on the quality of the labelled training data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model we are using is a classification model, as we want to make predictions to put individuals into 1 of 2 categories. \n",
    "# This is one example of a classification model, called a random forest classifier.\n",
    "# Others are available, which you can look up if you would like to. \n",
    "\n",
    "classification_model.fit(breast_cancer_inputs_training,breast_cancer_outputs_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is it, in just a few lines of code we have a machine learning model that can be used for predicting if someone has breast cancer!\n",
    "\n",
    "We can even try it out, using the actual use case of the model. In the future, doctors will likely use the model to put in information about 1 individual patient, and see whether they have a benign or cancerous tumour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataframe containing the information of one randomly sampled individual.\n",
    "individual_1_data = breast_cancer_inputs_testing.sample(n=1)\n",
    "display(individual_1_data)\n",
    "\n",
    "prediction = classification_model.predict(individual_1_data)\n",
    "print(prediction)\n",
    "# A 0 is a benign prediction, and a 1 is a cancerous prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But is our prediction right? We can check the label of this particular test record by peeking at the test outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Find out the individuals diagnosis\n",
    "individual_1_diagnosis = breast_cancer_outputs_testing[(individual_1_data.index)[0]]\n",
    "\n",
    "print(\"Individual 1 has a diagnosis of:\",individual_1_diagnosis)\n",
    "# A 0 is benign, a 1 is a cancerous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we now have the information for one individual. We also know what the actual outcome of the prediction should be.\n",
    "\n",
    "Next, we can use `predict`. This uses the model we have just trained to predict the results of our test set. It really is that simple!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the model get it right? It might have? But how do we know whether it will be correct every time? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### It's time to switch on our testing brains and start evaluating just how good this model is!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember the testing data we separated from the training data earlier? This is about 20% of the total data we have access to, and we can use it to evaluate just how good our model is. We can use the same `predict` function as above, but this time we can pass it far more data. \n",
    "\n",
    "Typically, you want **about** 80% of your data to be used for training, although a bit of variation from this is fine!. This gives you lots of data to train your model on, but still leaves enough for accurate testing. \n",
    "\n",
    "**Importantly**, this data is **labelled**. The basic premise of testing with labelled data is as follows, make your predictions (as we have just done), and then compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classification_model.predict(breast_cancer_inputs_testing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the predictions actually look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a really simple piece of analysis. How many of our predictions were correct? A super simple way to look at this is using a pie chart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib is another plotting library similar to plotly.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predictions(predictions, actual):\n",
    "    \"\"\"Creates a pie chart\n",
    "\n",
    "    Args:\n",
    "        predictions (Series): Predicted values by the model\n",
    "        actual (Series): Actual values\n",
    "    \"\"\"\n",
    "\n",
    "    correct_predictions = 0\n",
    "    incorrect_predictions = 0\n",
    "\n",
    "    for prediction, value in zip(predictions, actual):\n",
    "        if prediction == value:\n",
    "            correct_predictions += 1\n",
    "        else:\n",
    "            incorrect_predictions += 1\n",
    "\n",
    "    total_predictions = len(predictions)\n",
    "\n",
    "    # Remember our calculate_percentage function we made right at the start of the notebook!?\n",
    "    correct_percentage = round( calculate_percentage(correct_predictions,total_predictions) ,1)\n",
    "    incorrect_percentage = round( calculate_percentage(incorrect_predictions,total_predictions), 1)\n",
    "\n",
    "    # Plot the chart\n",
    "    plt.pie([correct_predictions,incorrect_predictions],\n",
    "            labels=[f\"Correct Predictions\\n{correct_percentage}\",f\"Incorrect Predictions\\n{incorrect_percentage}%\"])\n",
    "             \n",
    "    # Add a title\n",
    "    plt.title(\"Percentage of Correct and Incorrect Predictions\")\n",
    "    \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(predictions, breast_cancer_outputs_testing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other ways to analyse our results. For example using a confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def generate_confusion_matrix(predicted, actual, axis_labels = [\"Benign\", \"Malignant\"]):\n",
    "    \"\"\"Creates a confusion matrix chart\n",
    "\n",
    "    Args:\n",
    "        predictions (Series): Predicted values by the model\n",
    "        actual (Series): Actual values\n",
    "        axis_labels (list[string]): axis labels \n",
    "    \"\"\"\n",
    "    confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
    "\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = axis_labels)\n",
    "\n",
    "    cm_display.plot()\n",
    "\n",
    "    plt.title(\"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confusion_matrix(predictions, breast_cancer_outputs_testing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this actually show? \n",
    "\n",
    "The matrix has four squares, each representing a different combination of actual and predicted classifications. The squares are:\n",
    "\n",
    "- True Positive (TP): The model correctly predicted the positive diagnosis.\n",
    "- False Positive (FP): The model incorrectly predicted the positive diagnosis.\n",
    "- True Negative (TN): The model correctly predicted the negative diagnosis.\n",
    "- False Negative (FN): The model incorrectly predicted the negative diagnosis.\n",
    "\n",
    "The top left and bottom right boxes indicate correct predictions, but a confusion matrix allows us to see what kind of errors we're making. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some scenarios, an algorithm may want to be intentionally biased.\n",
    "\n",
    "- **Fire Alarms**: You would like a fire alarm to catch all fires. You would much rather a fire alarm go off when there is not a fire, than not go off when there is one. For this, we would like **high sensitivity** and **low specificity**. i.e. You want no false negatives but you'll accept some false positives.\n",
    "\n",
    "- **Spam Filters**: You do not want a spam filter to filter out important real emails! You care a lot about a **high specificity** and a **low sensitivity**. Therefore, you'll accept some false negatives but don't want any false positives.  \n",
    "\n",
    "![image](.//task.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a think - what bias would we prefer for this dataset and model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets go even further with our analysis and cover something called an AUC-ROC Curve. \n",
    "\n",
    "Firstly, what does AUC and ROC mean? \n",
    "\n",
    "- ROC: Receiver Operating Characteristics\n",
    "- AUC: Area Under Curve\n",
    "\n",
    "ROC essentially is a graphical representation of the effectiveness of a binary classification model.\n",
    "\n",
    "It plots the true positive rate (TPR) vs the false positive rate (FPR) at different classification thresholds.\n",
    "\n",
    "The AUC curve represents the area under the ROC curve. It measures the overall performance of the binary classification model. The\n",
    "\n",
    "The True Positive Rate (often called Recall or Sensitivity) is the ratio of positive examples that are correctly identified.\n",
    "\n",
    "`TPR = TP / TP + FN`\n",
    "\n",
    "FPR is the ratio of negative examples that are incorrectly classified.\n",
    "\n",
    "`FPR = FP / TN + FP`\n",
    "\n",
    "Lets make a plot to hopefully make more sense of all this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = metrics.RocCurveDisplay.from_estimator(classification_model, breast_cancer_inputs_testing, breast_cancer_outputs_testing)\n",
    "plt.plot([0,1],[0,1],\"--\",c=\"red\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The further to the top left corner the ROC curve is, the better the prediction.\n",
    "\n",
    "When the curve is perfectly in the top left corner, the area underneath it is 1. this means an AUC score of 1 is a perfect classifier, with 0 being the worse score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **It doesn't always need to be so complicated! Sometimes we just want a single number to help our evaluation!**\n",
    "\n",
    "Another method of analysis is an **F1 Score**. This is a more general measurement for how good a classifier model is.\n",
    "\n",
    "This is given as:\n",
    "\n",
    "**F1 = 2 * (precision * recall) / (precision + recall)**\n",
    "\n",
    "Where:\n",
    "- **Precision** is a measure of how many predicted positive diagnosis' are actually positive. This is found by taking the number of true positives and dividing by the number of true and false positives.\n",
    "- **Recall** is the percentage of positive cases that are correctly predicted by the model.\n",
    "\n",
    "A perfect prediction would give an F1 score of 1, with the worst possible score being 0. \n",
    "\n",
    "The sklearn classification report is a very useful way of examining the precision, recall and f1 score of **each class** (i.e benign or malignant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report = metrics.classification_report(breast_cancer_outputs_testing, predictions)\n",
    "\n",
    "print(classification_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](.//task.png)\n",
    "\n",
    "**OPTIONAL**\n",
    "\n",
    "So you have made predictions using the random forest algorithm, but could you use something else instead?\n",
    "\n",
    "Head to https://scikit-learn.org/stable/supervised_learning.html and see what other models are available.\n",
    "\n",
    "Can you find any better classification models for our dataset?\n",
    "\n",
    "Feel free to go back and look at the previous code in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTIONAL: Your code here!\n",
    "# import your model \n",
    "# fit to the training data\n",
    "# get predictions from the test data\n",
    "# test your model\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interrogating Training Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a tester, it is very important to look at training data as well as the predictions of a model.\n",
    "\n",
    "##### Why should someone assuring a model care about checking the training data it came from?\n",
    "\n",
    "- Being able to check the training data of a model is a key way to spot any flaws in the data\n",
    "- Flaws in the training data will be reflected in the predictive model.\n",
    "- Models may make incorrect predictions based on bad quality training data. \n",
    "\n",
    "One particular method we can use to check the training data is to look at **correlation** between variables.\n",
    "\n",
    "#### Why do we care about correlation?\n",
    "\n",
    " - Highly correlated variables essentially contain the same information.\n",
    " - When this happens, it becomes difficult for the model to distinguish the individual effects of each variable during training. This can lead to unstable and unreliable predictions.\n",
    " - Highly correlated variables can dominate the machine learning model.\n",
    " - Data Scientists often remove highly correlated variables, this ensures that each  variable contributes unique and independent information to the model, allowing for more accurate and reliable predictions.\n",
    " - However, we can't assume data scientists always do this! So it is important to check for ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just been given the training data for a regression model that predicts the diabetic progression of an individual. The data scientist claims he checked the training data before making the model, but we want to be sure! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Here, we load the diabetes inputs and outputs using the load diabetes function. \n",
    "\n",
    "# Earlier we used clear variables named breast_cancer_inputs and breast_cancer_outputs.\n",
    "\n",
    "# Sadly, data scientists often like to be more concise (and confusing), and like to use X and y.\n",
    "# You will likely see this a lot online, so it is good to practice this notation. \n",
    "\n",
    "diabetes_X, diabetes_y = load_diabetes(return_X_y=True, as_frame=True)\n",
    "# Here diabetes_X represents inputs, and diabetes_y represents outputs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(diabetes_X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = diabetes_X.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](.//task.png)\n",
    "\n",
    "This correlation plot shows the correlation between each variable/column.\n",
    "- Values close to 1 show high positive correlation.\n",
    "- Values close to -1 show high negative correlation.\n",
    "\n",
    "\n",
    "**Looking at this plot, which 2 columns show the highest correlation?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a tester, you may want to report this finding back to the developers.\n",
    "\n",
    "A data scientist would often remove columns that are highly correlated.\n",
    "\n",
    "In the code below, remove one of the columns to reduce highly correlated features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove a column\n",
    "# diabetes_X = diabetes_X.drop(\"put_the_name_of_a_column_here\", axis=1)\n",
    "#check the column is removed\n",
    "display(diabetes_X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there anything else looking a bit odd with this data? Maybe you have noticed that the `sex` values are either 0.050680 or -0.044642?\n",
    "\n",
    "That could be a bit odd, let's do some exploring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_value_counts = diabetes_X[\"sex\"].value_counts()\n",
    "print(sex_value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, it all looks pretty  *normal* . Firstly, there are only 2 values, which is a good sign! Secondly, it is normal for data scientists to apply transformations to training data to make it all a similar size, i.e. between 1 and -1. In this case, there doesn't seem to be a problem with the data, but it is always a good idea to keep your eyes peeled on training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Models\n",
    "\n",
    "A Data Scientist has used the above dataset to train a regression model. They have sent you the following code that they used to create a model to predict the progression of diabetes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_training, X_testing, y_training, y_testing = train_test_split(diabetes_X, diabetes_y, train_size=0.03)\n",
    "\n",
    "regression_model = LinearRegression()\n",
    "regression_model.fit(X_training,y_training)\n",
    "\n",
    "y_predictions = regression_model.predict(X_testing)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "mape = mean_absolute_percentage_error(y_testing, y_predictions)\n",
    "\n",
    "print(f\"Mean Absolute Percentage Error for model 1: {round(mape,2) * 100}%\")\n",
    "print(\"See, my model is amazing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First up, we've introduced a new testing metric, what is the mean absolute percentage error (MAPE)? It represents the average of the absolute percentage errors of each entry in a dataset. \n",
    "\n",
    "This means that, for each data point in the testing data, the absolute percentage error is calculated. These are then all added up together and averaged to come up with the MAPE number. The closer to 0 the MAPE, the better the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](.//task.png)\n",
    "\n",
    "Look again at the above code form the data scientist. There is **1** mistake / piece of bad practice that will reduce the accuracy of their code. Can you spot it? ðŸ‘€\n",
    "\n",
    "Fix the mistake and rerun the code, does the MAPE decrease?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance\n",
    "\n",
    "Feature Importance Analysis reveals how much each feature (columns/fields/dimensions) contributes to the model prediction.\n",
    "\n",
    "This can be used as a diagnostic tool:\n",
    "\n",
    "It reveals which features are barely used or not used at all to make a decision.\n",
    "It shows if the model is relying mostly or entirely on one or two features, this could suggest an error, or perhaps that a simpler model might be just as effective.\n",
    "\n",
    "More complex feature importance analysis can reveal the qualities of information that each feature gives.\n",
    "\n",
    "All of this can be used to diagnose issues within the training data, and help understand and explain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap is the module we wll use to run feature importance\n",
    "import shap \n",
    "# from sklearn.preprocessing import Imputer\n",
    "\n",
    "# we pass shap our classification model, alongside the training data\n",
    "explainer = shap.LinearExplainer(regression_model,masker=shap.maskers.Impute(data=X_training))\n",
    "# we then pass shap our testing inputs.\n",
    "shap_values = explainer.shap_values(X_testing)\n",
    "# shap then creates a plot for us showing which features are the most important\n",
    "shap.summary_plot(shap_values, X_testing, plot_type=\"bar\")\n",
    "\n",
    "# In the plot_type parameter, you may wish to try out \"violin\", \"dot\" or \"bar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](.//task.png)\n",
    "\n",
    "Before we move onto something new, see if you can answer these questions. From this plot, can you tell:\n",
    "\n",
    "- Which feature has the greatest impact on the model?\n",
    "\n",
    "- Which feature has the smallest impact on the model?\n",
    "\n",
    "- Is this what we expect to see? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning and Neural Networks!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep learning: **\n",
    "\n",
    "So far we've discussed machine learning using structured data. This means data that is tabulated and has labelled feature names.\n",
    "\n",
    "**Deep learning** is a more complex form of machine learning which generally involves **unstructured data** like images and natural language.\n",
    "\n",
    "**Deep learning** involves **Neural Networks** - computational models inspired by the human brain's neural connections. They are made up of interconnected nodes formed into layers.\n",
    "\n",
    "But the important thing to recognise is that a **Neural Network**, like any model, is just a complicated function, with an input, a hidden process, and an output.\n",
    "\n",
    "And the way we test them is much the same as before.\n",
    "\n",
    " \n",
    "\n",
    "You can think of the nodes and connections within the neural network as small, simple mathematical functions.\n",
    "\n",
    "As the neural network is trained, the weights associated with these small component parts are adjusted using a **loss function.**\n",
    "\n",
    "If the training data is good enough, the neural network will gradually be able to tend towards the desired predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](.//neural_network.png)\n",
    "\n",
    "### Let's make a dataset to try and classify with a neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs, make_swiss_roll\n",
    "import numpy as np\n",
    "\n",
    "# Use make swiss rolls to get the coordinates for two swiss roll shaped clusters\n",
    "X1, y1 = make_swiss_roll(n_samples=2000,noise=0.2, random_state=1)\n",
    "X2, y2 = make_swiss_roll(n_samples=2000,noise=0.2, random_state=2)\n",
    "#Use make blobs to get a \"blob\" shaped cluster\n",
    "X_blob,y_blob = make_blobs(n_samples = 2000,n_features=3, centers = 1, center_box=(10,0,0), random_state=4, cluster_std= 0.4)\n",
    "\n",
    "new_X = []\n",
    "new_y = []\n",
    "#Iterate through one cluster\n",
    "for i in range(len(X1)):\n",
    "    # Add the points from each cluster to a new array\n",
    "    new_X.append( list(X1[i]))\n",
    "    new_y.append(1)\n",
    "    new_X.append( [X2[i][0] * -1,X2[i][1] ,X2[i][2]* -1] ) # Apply some transformations to the coordinates\n",
    "    new_y.append(2)\n",
    "    new_X.append( [X_blob[i][0],X_blob[i][1] * 4 * np.random.rand() ,X_blob[i][2]] ) # Apply some transformations to the coordinates\n",
    "    new_y.append(y_blob[i] + 3)\n",
    "\n",
    "\n",
    "X = (pd.DataFrame(new_X))\n",
    "y = new_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our old produce_scatter_plot function to visualise the data.\n",
    "produce_scatter_plot(X,y,[0,1,2])\n",
    "\n",
    "# Interesting, as a human, we can clearly see the different groups, and would be able to classify them easily without colours. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into testing and training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we train a neural network?\n",
    "\n",
    "The full process for a training a neural network is very complicated, but it is useful to know a few key points. \n",
    "\n",
    "When training a neural network, you often pass the data to the model multiple times. Each time you pass the entire dataset to the model, this is called an **epoch**.\n",
    "\n",
    "A neural network measures it's training progress using a **loss function** that measures the difference between predicted outputs and actual targets. As you progress through multiple epochs, the loss function decreases\n",
    "\n",
    "During training, an **optimizer** algorithm to updates the model's parameters. The model updates the parameters based on methods called **gradient descent** and **back propagation** which are too complicated to go into today. But essentially, the model uses calculus to find the best possible value for each parameter in the model.  \n",
    "\n",
    "Often when training a model, we divide the dataset into smaller **batches** to process data efficiently.\n",
    "\n",
    "Let's see how we do this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# This line of code initialises a neural network called MLPClassifier\n",
    "# The max_iter parameter stands for the maximum number of iterations to go through.\n",
    "# This is the same as how many epochs to do, as we have set it to 1, we will go through all the data once.\n",
    "classifier = MLPClassifier(max_iter=1)\n",
    "\n",
    "# The fit function is the very clever bit of code that uses activation functions, gradient descent, an optimiser and back propagation to train the model.\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Very similar to before, we have a predict function. This takes the testing data and uses the model to make predictions\n",
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see what our predictions look like after one epoch\n",
    "produce_scatter_plot(X_test,predictions,[0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, it doesn't look very good does it!\n",
    "\n",
    "Let's train the model over multiple epochs and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# This line of code initialises the same neural network called MLPClassifier\n",
    "# It is here that we can change certain aspects of the model. For example I have changed the activation function to something called \"relu\".\n",
    "# The batch size is also set to 20, meaning 20 coordinates will be processed at a time.\n",
    "classifier = MLPClassifier(random_state=1, max_iter=20, batch_size=20, activation='relu')\n",
    "\n",
    "# I dont want my model to keep training forever, if the loss is no longer decreasing.\n",
    "# When the loss is no longer decreasing, the model is already well trained.\n",
    "# I will set up some code so that if the loss is decreasing by less than the tolerance, the training will stop.\n",
    "tolerance = 0.0001\n",
    "\n",
    "#Loop through up to 500 epoch\n",
    "for epoch in range(500):\n",
    "\n",
    "    # For each epoch, train the model on all of the data\n",
    "    # Partial fit is the same as the fit function, but ensures only one epoch is trained\n",
    "    classifier.partial_fit(X_train, y_train, classes = np.unique(y_train))\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = classifier.loss_curve_\n",
    "\n",
    "    if epoch > 2: \n",
    "        # Calculate the change in loss over the last two epochs\n",
    "        change_in_loss = (loss[-2] - loss[-1])\n",
    "    \n",
    "        if change_in_loss < tolerance:\n",
    "            # Check how small the change in loss is. If it's too small, stop training.\n",
    "            print(\"Change in loss is\",'{:0.6f}'.format(change_in_loss), \"which is less than the tolerance of\", tolerance)\n",
    "            print(f\"Stopping at epoch\", epoch)\n",
    "            break # this line breaks out of the loop if triggered\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see how good our prediction is after multiple epochs. \n",
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_scatter_plot(X_test,predictions,[0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! It might not be perfect but it is a lot better than before. \n",
    "\n",
    "Let's have a look at our loss against epoch to see how it changed during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)\n",
    "plt.xlabel(\"Epoch number\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the loss is a calculation that measures the difference between predicted outputs and actual targets.\n",
    "\n",
    "As the epochs continue, the loss decreases, meaning that the model is getting better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](.//task.png)\n",
    "\n",
    "Remember, although this is a neural network, it is still a classifier model! Can you use the plot_predictions function and generate_confusion_matrix functions to analyse this model? Further to this, can you generate a classification report?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(predictions,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confusion_matrix(predictions,y_test,[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "classification_report = metrics.classification_report(y_test, predictions)\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised vs Supervised Learning\n",
    "\n",
    "**Supervised learning** is a machine learning approach where the algorithm learns from labeled training data. The training data consists of input features (also called independent variables) and their corresponding known output labels (also called dependent variables or target variables). The goal of supervised learning is to learn a mapping function that can predict the correct output label for new, unseen input data. In other words, the algorithm learns from examples where the desired outcome is already known.\n",
    "\n",
    "**Unsupervised Learning**\n",
    "Unsupervised learning, on the other hand, is a machine learning approach where the algorithm learns from unlabeled data. Unlike supervised learning, there are no predefined output labels or target variables in unsupervised learning. The algorithm's objective is to find patterns, structures, or relationships within the data without any prior knowledge of the outcomes. Unsupervised learning algorithms attempt to uncover hidden patterns, group similar data points together, or reduce the dimensionality of the data without being guided by explicit labels.\n",
    "\n",
    "So far we have only used supervised learning, which is far more common. For the example above, many clustering algorithms exist which can group clusters together without using labelled data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Language Models\n",
    "\n",
    "Now these really are the talk of the town at the moment. The development of Large Language Models (LLM's) has exploded recently, leading to discussions of dystopian futures and robot overlords. Now whether this happens is quite debatable, but one thing for sure is that LLMs are going to change the way we work and our way of life in the future.\n",
    "\n",
    "Chat GPT is a LLM, and as I am sure you are all aware, it's very very impressive!\n",
    "\n",
    "Large Language Models are a type of Neural Network that are very good at processing language. \n",
    "\n",
    "One key aspect of LLM's is that they have a parameter called **temperature**. This essentially applies a sort of randomness to their answers. **LLM's can give multiple different answers to the same question**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
